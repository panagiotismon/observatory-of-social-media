{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de2865d-fcd4-4663-b5ad-7fbc3a882ab2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Libraries Intallation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59ddb8-c432-4f6b-a886-d25e62d3802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install sentencepiece\n",
    "# !pip install transformers datasets\n",
    "# !pip install transformers[torch]\n",
    "# !pip install accelerate\n",
    "# !pip install accelerate>=0.20.1\n",
    "# !pip install huggingface_hub\n",
    "# !pip install kaggle\n",
    "# !pip install tensorflow\n",
    "# !pip install scikit-learn\n",
    "# !pip install nvidia-pyindex\n",
    "# !pip install nvidia-tensorflow\n",
    "# !pip install tweet-preprocessor\n",
    "# !python3 -m pip install tensorflow[and-cuda]\n",
    "# !pip3 install pandas\n",
    "!{sys.executable} -m pip install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61844a62-d35a-444f-bb00-8ea9592aeaa5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66893346-de0d-44bb-8693-87cbd7b05070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 12:03:29.787422: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-17 12:03:29.787523: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-17 12:03:29.787546: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-17 12:03:29.794056: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/jamie/miniconda3/envs/tfgpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /home/jamie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import preprocessor as p\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e12d0-3af9-4bc7-82b1-d5d722147ead",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Set ENV Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f0004c-f3ff-411a-af60-83eba96f05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"evangeloskatsadouros\"\n",
    "os.environ['KAGGLE_KEY'] = \"cd793dfe3db43b66e0c59264b92465af\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b61b0-a661-45bd-a833-2bdb6c608827",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085eb606-86f5-4986-bfe5-0736f79ed0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(batch):\n",
    "    # tweet = batch[\"text\"]\n",
    "    # p.set_options(p.OPT.URL, p.OPT.EMOJI)\n",
    "    # tweet = p.clean(tweet)\n",
    "    # tweet = tweet.lower()\n",
    "\n",
    "    # tokenizer = TweetTokenizer()\n",
    "    # tokens = tokenizer.tokenize(tweet)\n",
    "    # # Remove special characters and punctuation\n",
    "    # tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    # # Remove stop words\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # tokens = [token for token in tokens if token not in stop_words]\n",
    "    # # Stemming using Porter Stemmer\n",
    "    # stemmer = PorterStemmer()\n",
    "    # tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # # Rejoin the tokens to form the preprocessed text\n",
    "    # preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return {'text':  batch[\"clean_text\"], 'label': batch[\"category\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3066a-0570-4341-bc18-bfb92bfe00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change label values -1 to 0 (for negative tweets), 0 to 1 (for neutral tweets) and 1 to 2 (for positive tweets)\n",
    "def transform_labels(labels):\n",
    "    transformed_labels = []\n",
    "    for label in labels:\n",
    "        if label==-1:\n",
    "            transformed_labels.append(0)\n",
    "        elif label==0:\n",
    "            transformed_labels.append(1)\n",
    "        else:\n",
    "            transformed_labels.append(2)\n",
    "\n",
    "    return transformed_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104ae6f6-c1de-4955-ad08-6dfeee651095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return roberta_tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c209ae8b-90e0-450e-b4bf-e17a49b0e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom metrics function\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p.predictions, p.label_ids\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(labels, predictions.argmax(axis=1), average='weighted')\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, predictions.argmax(axis=1))\n",
    "\n",
    "    return {\"f1\": f1, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b1d39-153f-408c-8334-6ac5ef7ec885",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133695e6-014a-4b6f-b103-488f7448e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=100)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Download Kaggle's Sentiment140 dataset (creaadits to https://www.kaggle.com/kazanova)\n",
    "!kaggle datasets download -d kazanova/sentiment140\n",
    "!unzip sentiment140.zip\n",
    "\n",
    "column_names = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "dataset_df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", names=column_names, encoding='latin-1', index_col=False)\n",
    "dataset_df = dataset_df.sample(frac = 1)\n",
    "\n",
    "# Split dataset into train and test lists\n",
    "total_items = len(dataset_df)\n",
    "train_point = math.floor(total_items*0.8)\n",
    "eval_point = math.floor((total_items-train_point)/2)+train_point\n",
    "\n",
    "train_texts = [preprocess_tweet(item) for item in list(dataset_df[\"text\"])[0:train_point]]\n",
    "train_targets = transform_labels(list(dataset_df[\"target\"])[0:train_point])\n",
    "eval_texts = [preprocess_tweet(item) for item in list(dataset_df[\"text\"])[train_point:eval_point]]\n",
    "eval_targets = transform_labels(list(dataset_df[\"target\"])[train_point:eval_point])\n",
    "test_texts = [preprocess_tweet(item) for item in list(dataset_df[\"text\"])[eval_point:]]\n",
    "test_targets = transform_labels(list(dataset_df[\"target\"])[eval_point:])\n",
    "\n",
    "# Load tokeanizer and train properties\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = SentimentDataset(train_texts, train_targets, roberta_tokenizer, 100)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_dataset = SentimentDataset(eval_texts, eval_targets, roberta_tokenizer, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8977bd-abc6-4996-8aab-dfc347008ceb",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6345c95b-b4a0-4b7c-add7-30fb84e35810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 162969/162969 [00:30<00:00, 5261.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "dataset = load_dataset(\"MaNaN-3/twitter_sentiment_analysis\").shuffle(seed=42)\n",
    "condition_to_remove = lambda example: (example['clean_text'] != None and example['category'] != None)\n",
    "dataset = dataset.filter(condition_to_remove)\n",
    "dataset = dataset.map(preprocess_tweet, remove_columns=['clean_text', 'category'], batched=True)\n",
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "340026d1-7cab-4b06-bfad-4069a7e925cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_eval = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "dataset_train_test = dataset_train_eval['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    'train': dataset_train_eval['train'],\n",
    "    'valid': dataset_train_test['train'],\n",
    "    'test': dataset_train_test['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbf2582b-5dcb-4598-bc93-d4054769f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=64,\n",
    "    num_train_epochs=2,  # Adjust the number of training epochs\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    save_steps=1000,  # Adjust the number of steps before saving the model\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6b8c75e-7304-494c-ad81-9b6285ee833f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4584' max='4584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4584/4584 31:29, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.374500</td>\n",
       "      <td>0.228861</td>\n",
       "      <td>0.926039</td>\n",
       "      <td>0.925994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.159692</td>\n",
       "      <td>0.953200</td>\n",
       "      <td>0.953240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>0.145140</td>\n",
       "      <td>0.957162</td>\n",
       "      <td>0.957045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.134880</td>\n",
       "      <td>0.962662</td>\n",
       "      <td>0.962445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>0.127636</td>\n",
       "      <td>0.965740</td>\n",
       "      <td>0.965758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.104151</td>\n",
       "      <td>0.970547</td>\n",
       "      <td>0.970545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.090298</td>\n",
       "      <td>0.975186</td>\n",
       "      <td>0.975209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.084782</td>\n",
       "      <td>0.975725</td>\n",
       "      <td>0.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.086448</td>\n",
       "      <td>0.977001</td>\n",
       "      <td>0.977050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4584, training_loss=0.14829644129122324, metrics={'train_runtime': 1890.3747, 'train_samples_per_second': 155.178, 'train_steps_per_second': 2.425, 'total_flos': 2.110465612773504e+16, 'train_loss': 0.14829644129122324, 'epoch': 2.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=None,\n",
    "    train_dataset=final_dataset['train'],\n",
    "    eval_dataset=final_dataset['valid'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33ae7d50-95db-4fbf-8199-036d81225a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1019' max='1019' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1019/1019 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.07872962951660156,\n",
       " 'eval_f1': 0.9766015398456451,\n",
       " 'eval_accuracy': 0.9765615412934102,\n",
       " 'eval_runtime': 20.6568,\n",
       " 'eval_samples_per_second': 394.495,\n",
       " 'eval_steps_per_second': 49.33,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(final_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8ac2402-74b2-4a7a-b90c-b3f9dd3e6103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(final_dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b320fb03-1dc7-42df-b202-80642eb47c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tf)",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
